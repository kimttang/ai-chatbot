import csv
import os
import requests
import json
import numpy as np
import unicodedata
from flask import Flask, request, jsonify, render_template_string
from collections import defaultdict
from konlpy.tag import Okt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# =========================================
# 1. ì„¤ì • ë° ì´ˆê¸°í™”
# =========================================
app = Flask(__name__)

TRANSLATION_CSV = "sentences.csv"
KNOWLEDGE_CSV = "company_docs.csv"
MODEL_NAME = "gemma3:4b"
API_URL = "http://localhost:11434/api/chat"

okt = Okt()

# [ë‡Œ 1] ë²ˆì—­ í†µê³„ (Dice Scoreìš©)
co_occurrence = defaultdict(lambda: defaultdict(int))
k_total_count = defaultdict(int)
e_total_count = defaultdict(int)

# ğŸš« í•™ìŠµìš© ë…¸ì´ì¦ˆ (í†µê³„ ì™œê³¡ ë°©ì§€)
LEARNING_NOISE = {
    # ê´€ì‚¬, ì „ì¹˜ì‚¬, beë™ì‚¬
    'a', 'an', 'the', 'this', 'that', 'it', 'there', 'here',
    'is', 'are', 'am', 'was', 'were', 'be', 'been',
    'to', 'in', 'on', 'at', 'of', 'for', 'with', 'by', 'from', 'up', 'out',

    # ì¡°ë™ì‚¬ (í•™ìŠµ ì œì™¸)
    'will', 'can', 'must', 'should', 'have', 'has', 'had',
    'do', 'does', 'did', 'done',

    # ëŒ€ëª…ì‚¬ (ì ìˆ˜ ë…ì‹ ë°©ì§€)
    'i', 'you', 'he', 'she', 'we', 'they', 'my', 'your', 'his', 'her', 'our', 'their',

    # ë¶€ì‚¬ ë° ì˜ë¯¸ ì—†ëŠ” ìˆ˜ì‹ì–´
    'so', 'its', 'very', 'really', 'just', 'currently',

    # [ì¤‘ìš”] í†µê³„ë¥¼ ë§ì¹˜ëŠ” íšŒì‚¬ ìš©ì–´ë“¤ (ë…¸ì´ì¦ˆ ë“±ë¡)
    'please', 'check', 'business', 'trip', 'scheduled', 'planning', 'share',
    'ran', 'running', 'due', 'mr', 'ms', 'homepage', 'enter', 'secret',
    'emails', 'company_staff_5g', 'let\'s', 'lets'
}

# ğŸ‘‘ VIP ê³ ì •ì„ (í†µê³„ ë¬´ì‹œí•˜ê³  ìµœìš°ì„  ì ìš©)
FIXED_MAPPING = {
    # ì§€ì‹œì‚¬ & ëŒ€ëª…ì‚¬
    'ì´ê²ƒ': 'this', 'ê·¸ê²ƒ': 'that', 'ì €ê²ƒ': 'that',
    'ì´': 'this', 'ê·¸': 'the', 'ì €': 'that', 'ì´ë²ˆ': 'this',
    'ì—¬ê¸°': 'here', 'ì €ê¸°': 'there',
    'ë‚˜': 'i', 'ë„ˆ': 'you', 'ìš°ë¦¬': 'we', 'ì œ': 'my', 'ë‚´': 'my',
    'ë¬´ì—‡': 'what', 'ëˆ„êµ¬': 'who', 'ì–¸ì œ': 'when', 'ì–´ë””': 'where', 'ì™œ': 'why', 'ì–´ë–»ê²Œ': 'how',

    # ë™ì‚¬ & ì„œìˆ ì–´
    'ìˆë‹¤': 'have', 'ì—†ë‹¤': 'not have',
    'ì´ë‹¤': 'is', 'ë‹¤': 'is', 'ì…ë‹ˆë‹¤': 'is', 'ì´ì—ìš”': 'is',
    'í•˜ë‹¤': 'do', 'í•©ë‹ˆë‹¤': 'do', 'í•´ìš”': 'do',
    'ì•Šë‹¤': 'not',

    # ìì£¼ í‹€ë¦¬ëŠ” ë™ì‚¬ ê³ ì •
    'ì¢‹ì•„í•˜ë‹¤': 'like', 'ë¨¹ë‹¤': 'eat', 'ê°€ë‹¤': 'go', 'ë³´ë‹¤': 'see',

    # ì‹œê°„ ê´€ë ¨
    'ì§€ê¸ˆ': 'now', 'ì˜¤ëŠ˜': 'today', 'ë‚´ì¼': 'tomorrow', 'ì–´ì œ': 'yesterday',
    'ë§¤ì¼': 'every day', 'ë§¤ì£¼': 'every week'
}

# [ë‡Œ 2] RAG ì €ì¥ì†Œ
rag_documents = []
vectorizer = None
doc_vectors = None
chat_history = []

# =========================================
# 2. í•œê¸€ ì •ê·œí™” (ìì†Œ ë¶„ë¦¬ ë°©ì§€)
# =========================================
def normalize_text(text):
    if not text: return ""
    # NFC: ììŒ+ëª¨ìŒì„ í•˜ë‚˜ë¡œ í•©ì¹¨
    text = unicodedata.normalize('NFC', text)
    return " ".join(text.strip().split())

# =========================================
# 3. ë°ì´í„° ë¡œë“œ (ëª¨ë“  ë°ì´í„° í•™ìŠµ)
# =========================================
def load_all_data():
    global vectorizer, doc_vectors, rag_documents

    # --- 1. ë²ˆì—­ ë°ì´í„° í•™ìŠµ ---
    print("âš™ï¸ [1/2] ë²ˆì—­ ë°ì´í„° ë¡œë“œ (ìŠ¹ì ë…ì‹ ëª¨ë“œ)...")
    if os.path.exists(TRANSLATION_CSV):
        co_occurrence.clear()
        k_total_count.clear()
        e_total_count.clear()

        lines = []
        try:
            with open(TRANSLATION_CSV, 'r', encoding='utf-8-sig') as f: lines = list(csv.DictReader(f))
        except:
            with open(TRANSLATION_CSV, 'r', encoding='cp949') as f: lines = list(csv.DictReader(f))

        for row in lines:
            kr = normalize_text(row.get("korean") or row.get("text") or "")
            en = row.get("english") or row.get("intent") or ""

            kr_tokens = okt.morphs(kr, stem=True)
            en_tokens = en.lower().replace(".", "").replace("?", "").replace(",", "").split()

            # ì¤‘ë³µ ì¹´ìš´íŠ¸ ë°©ì§€
            unique_k = set(k for k in kr_tokens if len(k) >= 1)
            unique_e = set(e for e in en_tokens if e not in LEARNING_NOISE)

            for k in unique_k:
                # 1ê¸€ìëŠ” ì œì™¸í•˜ë˜, 'íŒ€', 'ì¼' ë“±ì€ í—ˆìš©
                if len(k) < 2 and k not in ['ì¼', 'ì§‘', 'ë°©', 'ë¬¸', 'íŒ€', '3', '2']: continue
                k_total_count[k] += 1
            for e in unique_e:
                e_total_count[e] += 1
            for k in unique_k:
                if len(k) < 2 and k not in ['ì¼', 'ì§‘', 'ë°©', 'ë¬¸', 'íŒ€', '3', '2']: continue
                for e in unique_e:
                    co_occurrence[k][e] += 1

    # --- 2. íšŒì‚¬ ê·œì • ë¡œë“œ ---
    print("âš™ï¸ [2/2] íšŒì‚¬ ê·œì • ë¡œë“œ...")
    rag_documents = []
    corpus = []

    if os.path.exists(KNOWLEDGE_CSV):
        lines = []
        try:
            with open(KNOWLEDGE_CSV, 'r', encoding='utf-8-sig') as f: lines = list(csv.DictReader(f))
        except:
            print("âš ï¸ CP949 ëª¨ë“œë¡œ ì „í™˜í•˜ì—¬ ì½ìŠµë‹ˆë‹¤.")
            with open(KNOWLEDGE_CSV, 'r', encoding='cp949') as f: lines = list(csv.DictReader(f))

        for row in lines:
            q = normalize_text(row.get("text") or row.get("korean") or "")
            a = normalize_text(row.get("intent") or row.get("english") or "")
            if q and a:
                rag_documents.append({"text": q, "intent": a})
                corpus.append(q + " " + a)

        if corpus:
            vectorizer = TfidfVectorizer(preprocessor=normalize_text, analyzer='char_wb', ngram_range=(2, 4))
            doc_vectors = vectorizer.fit_transform(corpus)

    print("âœ… ëª¨ë“  ì¤€ë¹„ ì™„ë£Œ!")

# =========================================
# 4. [í•µì‹¬] ë²ˆì—­ ë¡œì§
# =========================================
def perform_strict_translation(text):
    target_text = normalize_text(text.replace("ë²ˆì—­", ""))

    # [ì¶”ê°€] ë­‰ì³ì„œ ìª¼ê°œì§€ëŠ” ê³ ìœ ëª…ì‚¬ ê°•ì œ ë¶„ë¦¬ (ê²½ì˜ì§€ì›íŒ€ -> ê²½ì˜ ì§€ì› íŒ€)
    target_text = target_text.replace("ê²½ì˜ì§€ì›íŒ€", "ê²½ì˜ ì§€ì› íŒ€")

    morphs = okt.morphs(target_text, stem=True)
    print(f"\nğŸ” [ë²ˆì—­ ë¶„ì„] {morphs}")

    # [1ë‹¨ê³„] ì² í†µ ë°©ì–´ (VIPëŠ” ë©´ì œ)
    missing_words = []
    for k_word in morphs:
        # VIPëŠ” í†µê³¼
        if k_word in FIXED_MAPPING: continue

        # ì¡°ì‚¬/ì–´ë¯¸/ë¶ˆìš©ì–´ í•„í„°ë§
        if k_word in ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'í•˜ë‹¤', 'ì´ë‹¤', 'ì˜', 'ìœ¼ë¡œ', 'ì™€', 'ê³¼', 'ê³ ', 'ë‹¤', 'ìš”', 'ê²ƒ', 'ìˆ˜',
                      'í•˜ê³ ', 'í•˜ëŠ”', 'ëœ', 'ë ', 'í• ', 'ì¸', 'ì ¸']:
            continue
        # 1ê¸€ì í•„í„°ë§
        if len(k_word) < 2 and k_word not in ['ì¼', 'ì§‘', 'ë°©', 'ë¬¸', 'íŒ€', '3', '2']: continue

        # í†µê³„ì— ì—†ìœ¼ë©´ ëª¨ë¥´ëŠ” ë‹¨ì–´
        if k_word not in co_occurrence:
            missing_words.append(k_word)

    if missing_words:
        return f"ğŸš« ë‹¤ìŒ ë‹¨ì–´ë¥¼ ë°°ìš´ ì ì´ ì—†ìŠµë‹ˆë‹¤: {', '.join(missing_words)}"

    # [2ë‹¨ê³„] ìŠ¹ì ë…ì‹ ë§¤ì¹­
    candidates = []
    for k_word in morphs:
        # VIP 1ìˆœìœ„
        if k_word in FIXED_MAPPING:
            candidates.append((2.0, k_word, FIXED_MAPPING[k_word]))
            continue

        # ë¶ˆìš©ì–´ í•„í„°ë§
        if k_word in ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'í•˜ë‹¤', 'ì´ë‹¤', 'ì˜', 'ìœ¼ë¡œ', 'ì™€', 'ê³¼', 'ê³ ', 'ë‹¤', 'ìš”', 'ê²ƒ', 'ìˆ˜',
                      'í•˜ê³ ', 'í•˜ëŠ”', 'ëœ', 'ë ', 'í• ', 'ì¸', 'ì ¸']: continue
        if len(k_word) < 2 and k_word not in ['ì¼', 'ì§‘', 'ë°©', 'ë¬¸', 'íŒ€', '3', '2']: continue

        # í†µê³„ ë§¤ì¹­
        mappings = co_occurrence.get(k_word)
        if mappings:
            for e_word, joint_count in mappings.items():
                # Dice Score
                score = (2 * joint_count) / (k_total_count[k_word] + e_total_count[e_word] + 0.1)
                candidates.append((score, k_word, e_word))

    # ì ìˆ˜ìˆœ ì •ë ¬
    candidates.sort(key=lambda x: x[0], reverse=True)

    final_keywords = []
    used_korean = set()
    used_english = set()

    for score, k_word, e_word in candidates:
        if k_word in used_korean: continue
        if e_word in used_english and not e_word.startswith("["): continue

        final_keywords.append(e_word)
        used_korean.add(k_word)
        used_english.add(e_word)
        print(f"   MATCH: {k_word} <-> {e_word} (ì ìˆ˜: {score:.2f})")

    keyword_str = ", ".join(final_keywords)
    print(f"ğŸ¤– [ìµœì¢… ì¬ë£Œ] {keyword_str}")

    # [3ë‹¨ê³„] Gemma ì¡°ë¦½ (í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€)
    prompt = (
        f"Task: Construct a simple English sentence using keywords: [{keyword_str}]\n"
        f"Rules:\n"
        f"1. Use ALL keywords provided.\n"
        f"2. IMPORTANT: Do NOT add new ideas or follow-up sentences. Keep it short.\n"
        f"3. If keywords are only nouns (e.g., 'this', 'computer'), use a simple structure like 'This is a computer'.\n"
        f"4. If a keyword like 'have' doesn't fit naturally, omit it.\n"
        f"5. Output ONLY the English sentence."
    )

    # ë²ˆì—­ì€ ê¸°ì–µë ¥ ë„ê³ (use_history=False) ì‹¤í–‰
    return call_ollama(prompt, system_role="grammar_corrector", use_history=False)

# =========================================
# 5. RAG ê²€ìƒ‰ ë¡œì§
# =========================================
def perform_rag_chat(text):
    text = normalize_text(text)
    print(f"\nğŸ” [RAG ê²€ìƒ‰] ì‚¬ìš©ì ì§ˆë¬¸: {text}")

    found = []

    # 1. ë²¡í„° ê²€ìƒ‰
    if vectorizer and doc_vectors is not None:
        try:
            query_vec = vectorizer.transform([text])
            sims = cosine_similarity(query_vec, doc_vectors)[0]
            ranked = np.argsort(-sims)[::-1]
            for i in ranked[:3]:
                if sims[i] > 0.15:
                    found.append(rag_documents[i])
        except: pass

    # 2. í‚¤ì›Œë“œ ê²€ìƒ‰ (ì ìˆ˜ì œ)
    if len(found) < 3:
        keywords = text.split()
        keyword_candidates = []
        for doc in rag_documents:
            match_count = 0
            for k in keywords:
                if len(k) > 1 and (k in doc['text'] or k in doc['intent']):
                    match_count += 1
            if match_count > 0:
                keyword_candidates.append((match_count, doc))

        keyword_candidates.sort(key=lambda x: x[0], reverse=True)
        for count, doc in keyword_candidates[:3]:
            if doc not in found:
                found.append(doc)

    if found:
        context = "\n".join([f"Q: {d['text']}\nA: {d['intent']}" for d in found[:3]])
    else:
        context = "ê´€ë ¨ëœ ë‚´ë¶€ ê·œì •ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    prompt = f"ì§ˆë¬¸: {text}"
    system_msg = f"ì°¸ê³  ë¬¸ì„œ:\n{context}\n\në¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”. ë‚´ìš©ì´ ì—†ìœ¼ë©´ 'ê´€ë ¨ ê·œì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µí•˜ì„¸ìš”."

    # RAGëŠ” ê¸°ì–µë ¥ ì¼œê³ (use_history=True) ì‹¤í–‰
    return call_ollama(prompt, system_role="assistant", context_msg=system_msg, use_history=True)

# =========================================
# 6. Ollama í˜¸ì¶œ
# =========================================
def call_ollama(user_msg, system_role="assistant", context_msg="", use_history=False):
    messages = [{"role": "system", "content": context_msg if context_msg else "You are a helpful assistant."}]

    if use_history and chat_history:
        messages.extend(chat_history[-4:])

    messages.append({"role": "user", "content": user_msg})

    try:
        response = requests.post(API_URL, json={"model": MODEL_NAME, "messages": messages, "stream": False}, timeout=60)
        response.raise_for_status()
        answer = response.json()['message']['content'].strip().replace('"', '')

        if use_history:
            chat_history.append({"role": "user", "content": user_msg})
            chat_history.append({"role": "assistant", "content": answer})

        return answer
    except Exception as e:
        return f"ì˜¤ë¥˜: {str(e)}"

# =========================================
# 7. ì›¹ ì„œë²„
# =========================================
HTML_TEMPLATE = """
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <title>ì„±ì¥í˜• AI í†µí•© ë´‡ (ìµœì¢…)</title>
    <style>
        body { font-family: 'Malgun Gothic', sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; background: #f4f4f9; }
        .container { background: white; padding: 20px; border-radius: 15px; box-shadow: 0 4px 10px rgba(0,0,0,0.1); }
        h2 { text-align: center; color: #333; }
        #chat-box { height: 500px; overflow-y: auto; border: 1px solid #ddd; padding: 20px; margin-bottom: 20px; background: #fff; border-radius: 8px; }
        .message { margin-bottom: 10px; padding: 10px 15px; border-radius: 20px; max-width: 80%; word-wrap: break-word; }
        .user { background: #007bff; color: white; margin-left: auto; text-align: right; border-bottom-right-radius: 2px; }
        .bot { background: #e9ecef; color: #333; margin-right: auto; text-align: left; border-bottom-left-radius: 2px; }
        .input-area { display: flex; gap: 10px; }
        input { flex: 1; padding: 15px; border: 1px solid #ddd; border-radius: 30px; outline: none; }
        button { padding: 15px 25px; background: #28a745; color: white; border: none; border-radius: 30px; cursor: pointer; font-weight: bold; }
        button:hover { background: #218838; }
        .tip { font-size: 12px; color: #666; text-align: center; margin-top: 10px; }
    </style>
</head>
<body>
    <div class="container">
        <h2>ğŸ¤– ì„±ì¥í˜• AI ë¹„ì„œ (Final)</h2>
        <div id="chat-box">
            <div class="message bot">ì•ˆë…•í•˜ì„¸ìš”! ê·œì • ì§ˆë¬¸ì´ë‚˜ ë²ˆì—­ ìš”ì²­ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.</div>
        </div>
        <div class="input-area">
            <input type="text" id="user-input" placeholder="ì§ˆë¬¸ ì…ë ¥ (ëì— 'ë²ˆì—­' ë¶™ì´ë©´ ë²ˆì—­ ëª¨ë“œ)" onkeypress="if(event.keyCode==13) sendMessage()">
            <button onclick="sendMessage()">ì „ì†¡</button>
        </div>
        <div class="tip">â€» ì˜ˆ: "ì—°ì°¨ ê·œì • ì•Œë ¤ì¤˜", "ë³´ê³ ì„œ ì œì¶œí–ˆì–´ ë²ˆì—­"</div>
    </div>
    <script>
        async function sendMessage() {
            const input = document.getElementById('user-input');
            const chatBox = document.getElementById('chat-box');
            const text = input.value.trim();
            if (!text) return;
            
            chatBox.innerHTML += `<div class="message user">${text}</div>`;
            input.value = '';
            chatBox.scrollTop = chatBox.scrollHeight;
            
            try {
                const res = await fetch('/chat', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify({ message: text })
                });
                const data = await res.json();
                chatBox.innerHTML += `<div class="message bot">${data.answer}</div>`;
            } catch (e) {
                chatBox.innerHTML += `<div class="message bot" style="color:red">ì˜¤ë¥˜ ë°œìƒ</div>`;
            }
            chatBox.scrollTop = chatBox.scrollHeight;
        }
    </script>
</body>
</html>
"""

@app.route('/')
def home():
    return render_template_string(HTML_TEMPLATE)

@app.route('/chat', methods=['POST'])
def chat():
    data = request.json
    user_input = data.get('message', '').strip()
    if user_input.endswith("ë²ˆì—­"):
        answer = perform_strict_translation(user_input)
    else:
        answer = perform_rag_chat(user_input)
    return jsonify({"answer": answer})

if __name__ == '__main__':
    load_all_data()
    print("ğŸš€ ì„œë²„ ì‹¤í–‰: http://localhost:5000")
    app.run(host='0.0.0.0', port=5000)